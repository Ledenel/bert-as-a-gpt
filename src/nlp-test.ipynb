{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('zh-nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1eeda655a9877d9044d02b650ae864055c340b0ee92bd995be48284403500603"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "config_dict = dict(\n",
    "    cache_dir=\"cache\",\n",
    "    # force_download=True,\n",
    "    # resume_download=True,\n",
    "    proxies={'http': os.environ[\"HTTP_PROXY\"], 'https': os.environ[\"HTTPS_PROXY\"]}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\", **config_dict)\n",
    "model = AutoModelWithLMHead.from_pretrained(\"bert-base-chinese\", **config_dict)\n",
    "\n",
    "sequence = f\"生活的真谛是{tokenizer.mask_token}。\"\n",
    "\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(input).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token(x):\n",
    "    return f\"{tokenizer.decode([x])}[{x}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[MASK] 生 活 的 真 谛 是 美 。 [SEP]\n",
      "[('[CLS][101]', -4.34677267074585),\n",
      " ('，[8024]', 5.548994064331055),\n",
      " ('。[511]', 5.360104560852051),\n",
      " ('的[4638]', 5.346104621887207),\n",
      " ('人[782]', 4.416329383850098),\n",
      " ('了[749]', 4.279936790466309)]\n",
      "[CLS] [MASK] 活 的 真 谛 是 美 。 [SEP]\n",
      "[('生[4495]', 19.903968811035156),\n",
      " ('生[4495]', 19.903968811035156),\n",
      " ('人[782]', 12.397592544555664),\n",
      " ('鲜[7831]', 10.195749282836914),\n",
      " ('美[5401]', 9.150812149047852),\n",
      " ('快[2571]', 9.078143119812012)]\n",
      "[CLS] 生 [MASK] 的 真 谛 是 美 。 [SEP]\n",
      "[('活[3833]', 19.629314422607422),\n",
      " ('活[3833]', 19.629314422607422),\n",
      " ('命[1462]', 19.119295120239258),\n",
      " ('存[2100]', 12.679109573364258),\n",
      " ('生[4495]', 11.334877014160156),\n",
      " ('物[4289]', 11.22317123413086)]\n",
      "[CLS] 生 活 [MASK] 真 谛 是 美 。 [SEP]\n",
      "[('的[4638]', 18.556386947631836),\n",
      " ('的[4638]', 18.556386947631836),\n",
      " ('之[722]', 13.935623168945312),\n",
      " ('，[8024]', 13.344372749328613),\n",
      " ('最[3297]', 10.701181411743164),\n",
      " ('其[1071]', 9.818900108337402)]\n",
      "[CLS] 生 活 的 [MASK] 谛 是 美 。 [SEP]\n",
      "[('真[4696]', 21.43470001220703),\n",
      " ('真[4696]', 21.43470001220703),\n",
      " ('美[5401]', 14.201088905334473),\n",
      " ('正[3633]', 11.537816047668457),\n",
      " ('底[2419]', 11.247458457946777),\n",
      " ('至[5635]', 11.212472915649414)]\n",
      "[CLS] 生 活 的 真 [MASK] 是 美 。 [SEP]\n",
      "[('谛[6465]', 13.196931838989258),\n",
      " ('谛[6465]', 13.196931838989258),\n",
      " ('实[2141]', 12.569402694702148),\n",
      " ('的[4638]', 12.011106491088867),\n",
      " ('實[2179]', 11.640177726745605),\n",
      " ('諦[6320]', 11.381936073303223)]\n",
      "[CLS] 生 活 的 真 谛 [MASK] 美 。 [SEP]\n",
      "[('是[3221]', 15.301570892333984),\n",
      " ('是[3221]', 15.301570892333984),\n",
      " ('，[8024]', 12.160385131835938),\n",
      " ('在[1762]', 11.18807601928711),\n",
      " ('：[8038]', 10.742080688476562),\n",
      " ('-[118]', 10.718381881713867)]\n",
      "[CLS] 生 活 的 真 谛 是 [MASK] 。 [SEP]\n",
      "[('美[5401]', 11.613923072814941),\n",
      " ('美[5401]', 11.613923072814941),\n",
      " ('爱[4263]', 11.217545509338379),\n",
      " ('乐[727]', 9.265684127807617),\n",
      " ('人[782]', 8.916584014892578),\n",
      " ('：[8038]', 8.749302864074707)]\n",
      "[CLS] 生 活 的 真 谛 是 美 [MASK] [SEP]\n",
      "[('。[511]', 18.797727584838867),\n",
      " ('。[511]', 18.797727584838867),\n",
      " ('！[8013]', 16.96658706665039),\n",
      " ('？[8043]', 12.56213092803955),\n",
      " ('，[8024]', 10.819461822509766),\n",
      " ('：[8038]', 8.298932075500488)]\n",
      "[CLS] 生 活 的 真 谛 是 美 。 [MASK]\n",
      "[('[SEP][102]', -4.715410232543945),\n",
      " ('生[4495]', 8.252614974975586),\n",
      " ('。[511]', 8.103643417358398),\n",
      " ('人[782]', 7.4958014488220215),\n",
      " ('活[3833]', 7.417153358459473),\n",
      " ('的[4638]', 7.393743515014648)]\n"
     ]
    }
   ],
   "source": [
    "sequence = f\"生活的真谛是美。\"\n",
    "from pprint import pprint\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "for mask_token_index, real_token in enumerate(input[0]):\n",
    "    masked_input = input.clone()\n",
    "    masked_input[0, mask_token_index] = tokenizer.mask_token_id\n",
    "    print(tokenizer.decode(masked_input[0]))\n",
    "    token_logits = model(masked_input).logits\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5).indices.tolist()\n",
    "    pprint([(print_token(x),float(mask_token_logits[x])) for x in [real_token] + top_5_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylist import MyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0588,  0.0704, -0.2139,  ..., -0.0237, -0.2234, -0.1116],\n",
       "          [-0.2540,  0.1951, -0.9774,  ..., -1.0265, -0.0109,  1.4410],\n",
       "          [-0.4060, -0.2025,  0.6814,  ..., -0.6053,  0.9638,  0.6932],\n",
       "          ...,\n",
       "          [ 0.1026,  0.4093,  0.0582,  ..., -0.5345, -0.1433,  0.1866],\n",
       "          [ 0.1287,  0.6613,  0.3791,  ..., -0.8779, -0.4095,  1.0400],\n",
       "          [ 0.1603,  0.2365,  0.2230,  ..., -0.7591, -0.5075,  0.5007]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.0819,  0.0069, -0.2078,  ...,  0.0875, -0.0651, -0.0418],\n",
       "          [-0.0742, -0.5346, -0.8241,  ..., -1.2033, -0.1132,  0.8679],\n",
       "          [ 0.3037,  0.0661,  0.8634,  ..., -0.3258,  1.1006,  0.4839],\n",
       "          ...,\n",
       "          [-0.0783,  0.0824, -0.3625,  ..., -0.6974,  0.1537,  0.4883],\n",
       "          [ 0.0236,  0.4476, -0.1046,  ..., -0.7037, -0.1806,  0.9855],\n",
       "          [ 0.0296,  0.3524,  0.1097,  ..., -0.9359, -0.3817,  0.5382]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-5.1074e-03,  5.9234e-02, -1.2732e-01,  ...,  4.4828e-02,\n",
       "           -6.2009e-02,  4.0345e-02],\n",
       "          [-5.2224e-01, -2.5344e-01, -4.6002e-01,  ..., -7.0365e-01,\n",
       "           -5.2078e-01,  2.5293e-01],\n",
       "          [ 1.4505e-01,  8.0913e-01,  1.0570e+00,  ...,  1.1059e-01,\n",
       "            9.8386e-01,  2.1507e-01],\n",
       "          ...,\n",
       "          [-7.7584e-03, -1.0121e-01, -1.4740e-01,  ..., -5.2610e-01,\n",
       "           -3.4379e-02,  2.5270e-01],\n",
       "          [-2.3753e-03,  5.3938e-06, -3.3402e-01,  ..., -3.1457e-01,\n",
       "           -8.3954e-02,  4.3524e-01],\n",
       "          [ 4.5221e-02,  5.9472e-02,  1.6604e-01,  ..., -1.6442e-01,\n",
       "           -2.1228e-01, -1.7231e-01]]], grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 9.8881e-02,  1.5844e-01, -9.3900e-02,  ...,  2.2962e-01,\n",
       "           -2.7336e-01,  8.6420e-02],\n",
       "          [-2.4395e-01, -2.2308e-01, -2.6533e-01,  ..., -5.4372e-01,\n",
       "           -9.5606e-01,  4.0375e-01],\n",
       "          [ 5.7766e-01,  6.3479e-01,  9.5981e-01,  ...,  4.5657e-01,\n",
       "            7.5831e-01,  4.0257e-01],\n",
       "          ...,\n",
       "          [ 1.2726e-01, -3.6355e-01, -5.1246e-01,  ..., -7.4411e-01,\n",
       "            2.6342e-01,  2.7503e-01],\n",
       "          [ 7.0985e-03, -2.0679e-01, -6.3003e-01,  ...,  1.5160e-01,\n",
       "            9.9977e-02,  2.7142e-01],\n",
       "          [ 5.8406e-04, -2.4834e-02, -6.1981e-03,  ..., -7.4106e-02,\n",
       "           -8.2120e-02, -5.3700e-02]]], grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.1784,  0.1219, -0.1758,  ...,  0.3473, -0.3633,  0.1534],\n",
       "          [-0.0121, -0.7936, -0.1196,  ..., -0.9516, -0.5812,  0.3303],\n",
       "          [ 1.0543,  0.3763,  1.0784,  ..., -0.2509,  0.8346,  0.1423],\n",
       "          ...,\n",
       "          [ 0.4721, -0.1172, -0.5880,  ..., -0.6650,  0.2557, -0.1096],\n",
       "          [ 0.4264, -0.1110, -0.5018,  ...,  0.0277,  0.0947,  0.1686],\n",
       "          [-0.0367, -0.0647,  0.0364,  ..., -0.0673, -0.0675, -0.0478]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.0148,  0.3396,  0.0728,  ...,  0.2847, -0.3226,  0.2655],\n",
       "          [ 0.0427,  0.0267, -0.4839,  ..., -0.0579, -0.3306,  0.6489],\n",
       "          [ 0.7814,  0.6854,  0.7537,  ...,  0.3338,  0.8429,  0.8758],\n",
       "          ...,\n",
       "          [ 0.6100, -0.2884, -0.6155,  ..., -0.5239,  0.3283,  0.0427],\n",
       "          [ 0.7511,  0.2423, -0.2225,  ..., -0.4691,  0.4956,  0.1797],\n",
       "          [-0.0593, -0.0574, -0.0042,  ..., -0.0599, -0.0578, -0.0623]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 2.4526e-01,  6.4084e-01,  5.7842e-01,  ...,  4.7959e-01,\n",
       "           -3.5527e-01,  3.0752e-01],\n",
       "          [ 1.9190e-01,  2.5004e-01, -5.7473e-01,  ...,  4.5755e-01,\n",
       "           -6.4627e-01,  3.5074e-01],\n",
       "          [ 6.9202e-01,  7.0624e-01,  3.9232e-01,  ...,  5.8101e-01,\n",
       "            6.7345e-01,  7.3693e-01],\n",
       "          ...,\n",
       "          [ 8.4100e-01, -2.7189e-01, -5.6697e-01,  ..., -1.7558e-01,\n",
       "            9.8118e-02, -6.0915e-02],\n",
       "          [ 8.7545e-01, -3.9565e-04, -2.4045e-01,  ..., -1.4620e-01,\n",
       "            2.1503e-01, -1.7755e-01],\n",
       "          [-6.3288e-02, -4.4038e-02, -2.8590e-03,  ..., -2.4714e-02,\n",
       "           -3.0205e-02, -6.9223e-02]]], grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.0889,  0.3829,  0.5660,  ...,  0.6951, -0.4607,  0.4577],\n",
       "          [-0.1434,  0.2752, -0.3527,  ...,  0.7275, -1.0025,  0.3781],\n",
       "          [ 0.4119,  0.2030,  0.2968,  ...,  0.6543, -0.1816,  0.6670],\n",
       "          ...,\n",
       "          [ 1.0459, -0.3515, -0.9492,  ..., -0.5142,  0.3230, -0.6318],\n",
       "          [ 0.6936, -0.4912,  0.0174,  ...,  0.3706, -0.1852, -0.1283],\n",
       "          [ 0.0018, -0.0688, -0.0050,  ..., -0.0042, -0.0409, -0.0424]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.1063,  0.6113,  0.5207,  ...,  0.4133, -0.4544,  0.3320],\n",
       "          [ 0.1197,  0.0716, -0.3122,  ...,  0.8054, -0.6450,  0.1512],\n",
       "          [ 0.7273,  0.0144,  0.0958,  ...,  0.5856,  0.1086,  0.2575],\n",
       "          ...,\n",
       "          [ 0.6675, -0.6360, -1.2409,  ..., -0.3950,  0.0475, -0.5515],\n",
       "          [ 0.3666,  0.1105,  0.1457,  ...,  0.4990, -0.2150,  0.1456],\n",
       "          [-0.0022, -0.0725, -0.0143,  ..., -0.0253, -0.0246, -0.0719]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.4433,  0.6109,  0.4403,  ...,  0.9529, -0.2335,  0.6886],\n",
       "          [ 0.5010, -0.1432,  0.1081,  ...,  0.8864, -0.5593,  0.1752],\n",
       "          [ 1.1611, -0.2520,  0.2678,  ...,  1.0427, -0.0279,  0.2199],\n",
       "          ...,\n",
       "          [ 0.7143, -0.5987, -0.9414,  ...,  0.2182,  0.5614, -0.2814],\n",
       "          [ 0.7729,  0.3049,  0.2290,  ...,  0.7551,  0.2756,  0.4826],\n",
       "          [ 0.0013, -0.0511,  0.0024,  ..., -0.0287, -0.0043, -0.0569]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 1.0669,  0.3184,  0.1179,  ...,  0.8310,  0.3503,  0.3731],\n",
       "          [ 1.3740, -0.4871,  0.4318,  ...,  0.7985, -0.0683,  0.1233],\n",
       "          [ 1.8175, -0.5181,  0.4539,  ...,  1.1192,  0.2627,  0.2509],\n",
       "          ...,\n",
       "          [ 0.4586, -0.1871, -0.9517,  ...,  0.7180,  0.1360, -0.1604],\n",
       "          [ 1.2191,  0.3932,  0.2980,  ...,  0.8076,  0.4565,  0.3372],\n",
       "          [-0.0540, -0.0497,  0.0213,  ..., -0.0373,  0.0235, -0.0620]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 1.2920,  0.2658,  0.1234,  ...,  0.9263,  0.3891,  0.3786],\n",
       "          [ 1.3272, -0.2320,  0.6741,  ...,  0.2492,  0.1102,  0.1667],\n",
       "          [ 1.8015, -0.6413,  0.5328,  ...,  0.9738,  0.3930,  0.3068],\n",
       "          ...,\n",
       "          [ 0.7938, -0.3292, -0.5076,  ...,  1.0153, -0.1458, -0.2456],\n",
       "          [ 1.1509,  0.2105,  0.4257,  ...,  0.4298,  0.5110,  0.5285],\n",
       "          [-0.0511, -0.0301,  0.0425,  ..., -0.0180,  0.0522, -0.0506]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 1.2148,  0.1328,  0.2446,  ...,  0.5536,  0.3042,  0.3785],\n",
       "          [ 0.4214, -0.0083,  1.0266,  ..., -0.1193, -0.1996,  0.2399],\n",
       "          [ 1.1916, -0.5252,  0.1210,  ...,  1.3906,  0.5296, -0.0643],\n",
       "          ...,\n",
       "          [ 0.6016, -0.8475, -0.5942,  ...,  0.9596, -0.0281, -0.1563],\n",
       "          [ 1.1536,  0.5546,  0.3465,  ...,  0.3436,  0.4000,  0.3713],\n",
       "          [ 0.8235, -0.1969,  0.3804,  ...,  0.2602, -0.5121,  0.0856]]],\n",
       "        grad_fn=<NativeLayerNormBackward>))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model(input, output_hidden_states=True).hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_decode',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_from_pretrained',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'truncate_sequences',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[unused1]', '[unused2]', '[unused3]']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "output_type": "stream",
     "text": [
      "tokenizer.vocab\n",
      "tokenizer.vocab_files_names\n",
      "tokenizer.vocab_size"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "tokenizer.voca*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1235       勇\n",
       "2359       帆\n",
       "5169       細\n",
       "7187       铀\n",
       "14316    ##包\n",
       "        ... \n",
       "14903    ##塊\n",
       "18099    ##简\n",
       "7802       鮪\n",
       "6470       谤\n",
       "16773    ##氲\n",
       "Length: 21128, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}