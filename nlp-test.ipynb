{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('zh-nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1eeda655a9877d9044d02b650ae864055c340b0ee92bd995be48284403500603"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "config_dict = dict(\n",
    "    cache_dir=\"cache\",\n",
    "    # force_download=True,\n",
    "    # resume_download=True,\n",
    "    proxies={'http': os.environ[\"HTTP_PROXY\"], 'https': os.environ[\"HTTPS_PROXY\"]}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\", **config_dict)\n",
    "model = AutoModelWithLMHead.from_pretrained(\"bert-base-chinese\", **config_dict)\n",
    "\n",
    "sequence = f\"生活的真谛是{tokenizer.mask_token}。\"\n",
    "\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(input).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token(x):\n",
    "    return f\"{tokenizer.decode([x])}[{x}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[MASK] 生 活 的 真 谛 是 美 。 [SEP]\n",
      "[('[CLS][101]', -4.34677267074585),\n",
      " ('，[8024]', 5.548994064331055),\n",
      " ('。[511]', 5.360104560852051),\n",
      " ('的[4638]', 5.346104621887207),\n",
      " ('人[782]', 4.416329383850098),\n",
      " ('了[749]', 4.279936790466309)]\n",
      "[CLS] [MASK] 活 的 真 谛 是 美 。 [SEP]\n",
      "[('生[4495]', 19.903968811035156),\n",
      " ('生[4495]', 19.903968811035156),\n",
      " ('人[782]', 12.397592544555664),\n",
      " ('鲜[7831]', 10.195749282836914),\n",
      " ('美[5401]', 9.150812149047852),\n",
      " ('快[2571]', 9.078143119812012)]\n",
      "[CLS] 生 [MASK] 的 真 谛 是 美 。 [SEP]\n",
      "[('活[3833]', 19.629314422607422),\n",
      " ('活[3833]', 19.629314422607422),\n",
      " ('命[1462]', 19.119295120239258),\n",
      " ('存[2100]', 12.679109573364258),\n",
      " ('生[4495]', 11.334877014160156),\n",
      " ('物[4289]', 11.22317123413086)]\n",
      "[CLS] 生 活 [MASK] 真 谛 是 美 。 [SEP]\n",
      "[('的[4638]', 18.556386947631836),\n",
      " ('的[4638]', 18.556386947631836),\n",
      " ('之[722]', 13.935623168945312),\n",
      " ('，[8024]', 13.344372749328613),\n",
      " ('最[3297]', 10.701181411743164),\n",
      " ('其[1071]', 9.818900108337402)]\n",
      "[CLS] 生 活 的 [MASK] 谛 是 美 。 [SEP]\n",
      "[('真[4696]', 21.43470001220703),\n",
      " ('真[4696]', 21.43470001220703),\n",
      " ('美[5401]', 14.201088905334473),\n",
      " ('正[3633]', 11.537816047668457),\n",
      " ('底[2419]', 11.247458457946777),\n",
      " ('至[5635]', 11.212472915649414)]\n",
      "[CLS] 生 活 的 真 [MASK] 是 美 。 [SEP]\n",
      "[('谛[6465]', 13.196931838989258),\n",
      " ('谛[6465]', 13.196931838989258),\n",
      " ('实[2141]', 12.569402694702148),\n",
      " ('的[4638]', 12.011106491088867),\n",
      " ('實[2179]', 11.640177726745605),\n",
      " ('諦[6320]', 11.381936073303223)]\n",
      "[CLS] 生 活 的 真 谛 [MASK] 美 。 [SEP]\n",
      "[('是[3221]', 15.301570892333984),\n",
      " ('是[3221]', 15.301570892333984),\n",
      " ('，[8024]', 12.160385131835938),\n",
      " ('在[1762]', 11.18807601928711),\n",
      " ('：[8038]', 10.742080688476562),\n",
      " ('-[118]', 10.718381881713867)]\n",
      "[CLS] 生 活 的 真 谛 是 [MASK] 。 [SEP]\n",
      "[('美[5401]', 11.613923072814941),\n",
      " ('美[5401]', 11.613923072814941),\n",
      " ('爱[4263]', 11.217545509338379),\n",
      " ('乐[727]', 9.265684127807617),\n",
      " ('人[782]', 8.916584014892578),\n",
      " ('：[8038]', 8.749302864074707)]\n",
      "[CLS] 生 活 的 真 谛 是 美 [MASK] [SEP]\n",
      "[('。[511]', 18.797727584838867),\n",
      " ('。[511]', 18.797727584838867),\n",
      " ('！[8013]', 16.96658706665039),\n",
      " ('？[8043]', 12.56213092803955),\n",
      " ('，[8024]', 10.819461822509766),\n",
      " ('：[8038]', 8.298932075500488)]\n",
      "[CLS] 生 活 的 真 谛 是 美 。 [MASK]\n",
      "[('[SEP][102]', -4.715410232543945),\n",
      " ('生[4495]', 8.252614974975586),\n",
      " ('。[511]', 8.103643417358398),\n",
      " ('人[782]', 7.4958014488220215),\n",
      " ('活[3833]', 7.417153358459473),\n",
      " ('的[4638]', 7.393743515014648)]\n"
     ]
    }
   ],
   "source": [
    "sequence = f\"生活的真谛是美。\"\n",
    "from pprint import pprint\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "for mask_token_index, real_token in enumerate(input[0]):\n",
    "    masked_input = input.clone()\n",
    "    masked_input[0, mask_token_index] = tokenizer.mask_token_id\n",
    "    print(tokenizer.decode(masked_input[0]))\n",
    "    token_logits = model(masked_input).logits\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5).indices.tolist()\n",
    "    pprint([(print_token(x),float(mask_token_logits[x])) for x in [real_token] + top_5_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylist import MyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}